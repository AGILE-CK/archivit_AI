{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openprompt in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: transformers>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (4.36.2)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (0.1.96)\n",
      "Requirement already satisfied: tqdm>=4.62.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (4.66.1)\n",
      "Requirement already satisfied: tensorboardX in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (2.6.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (3.8.1)\n",
      "Requirement already satisfied: yacs in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (0.1.8)\n",
      "Requirement already satisfied: dill in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (0.3.7)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (2.16.1)\n",
      "Requirement already satisfied: rouge==1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (1.0.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (14.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openprompt) (1.11.4)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=4.10.0->openprompt) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.10.0->openprompt) (0.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->openprompt) (0.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->openprompt) (2.1.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->openprompt) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->openprompt) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->openprompt) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->openprompt) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->openprompt) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->openprompt) (1.3.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboardX->openprompt) (4.23.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->openprompt) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->openprompt) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.10.0->openprompt) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->openprompt) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->openprompt) (2023.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "from openprompt.data_utils import InputExample\n",
    "import copy\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Label\n",
       "0  denial of normal the con be asked to comment o...      1\n",
       "1  just by being able to tweet this insufferable ...      1\n",
       "2  that is retarded you too cute to be single tha...      1\n",
       "3  thought of a real badass mongol style declarat...      1\n",
       "4                                afro american basho      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"HateSpeechDatasetBalanced.csv\")\n",
    "\n",
    "# Extract the needed columns\n",
    "df = df[['Content', 'Label']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1    364526\n",
       "0    361597\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    726123.000000\n",
       "mean        196.846960\n",
       "std         247.350801\n",
       "min           1.000000\n",
       "25%          59.000000\n",
       "50%         109.000000\n",
       "75%         228.000000\n",
       "max        9998.000000\n",
       "Name: Content, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726110</th>\n",
       "      <td>banning bare breasts ha gayyyyyy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726119</th>\n",
       "      <td>I'm gonna kill you</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726120</th>\n",
       "      <td>Someone is better than us</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726121</th>\n",
       "      <td>someone is really</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726122</th>\n",
       "      <td>really</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412122 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Content  Label\n",
       "0       denial of normal the con be asked to comment o...      1\n",
       "1       just by being able to tweet this insufferable ...      1\n",
       "2       that is retarded you too cute to be single tha...      1\n",
       "3       thought of a real badass mongol style declarat...      1\n",
       "4                                     afro american basho      1\n",
       "...                                                   ...    ...\n",
       "726110                   banning bare breasts ha gayyyyyy      1\n",
       "726119                                 I'm gonna kill you      1\n",
       "726120                          Someone is better than us      0\n",
       "726121                                  someone is really      0\n",
       "726122                                             really      0\n",
       "\n",
       "[412122 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Content'].str.len() < 128]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split DataFrame\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)  # 80% train, 20% validation\n",
    "\n",
    "# Subset, this can be adjust\n",
    "train_df = train_df.iloc[:3600, :]\n",
    "valid_df = valid_df.iloc[:3600, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initialize RoBERTa Tokenizer\n",
    "\n",
    "the roberta is better than bert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PLM, tokenizer, and wrapper class\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Hateful Data size: 178229\n",
      "Hateful Data size: 233893\n"
     ]
    }
   ],
   "source": [
    "hateful_df = df[df['Label'] == 1]\n",
    "non_hateful_df = df[df['Label'] == 0]\n",
    "\n",
    "print(\"Non-Hateful Data size:\", non_hateful_df.shape[0])\n",
    "print(\"Hateful Data size:\", hateful_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # Random choice training set example with label 0 (non-hateful)\n",
    "    random_example_1 = non_hateful_df.sample(1).iloc[0]['Content']\n",
    "    # Random choice training set example with label 1 (hateful)\n",
    "    random_example_2 = hateful_df.sample(1).iloc[0]['Content']\n",
    "\n",
    "    demonstration = f'{random_example_1} Sentence for analyze: This sentence is positive.'\\\n",
    "                    f' '\\\n",
    "                    f'{random_example_2} Sentence for analyze: This sentence is negative.'\n",
    "    demonstrations.append(demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ManualTemplate(\n",
    "    tokenizer=tokenizer, \n",
    "    text='{\"placeholder\":\"text_a\"} Sentence for analyze: This sentence is {\"mask\"}.' \n",
    "        + ' '.join(demonstrations)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizer = ManualVerbalizer(\n",
    "    tokenizer=tokenizer, \n",
    "    num_classes=2, \n",
    "    label_words=['positive','negative']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'the national organization for women should be banned it is not exactly like women deserve rights anyway',\n",
       "   'loss_ids': 0,\n",
       "   'shortenable_ids': 1},\n",
       "  {'text': ' Sentence for analyze: This sentence is',\n",
       "   'loss_ids': 0,\n",
       "   'shortenable_ids': 0},\n",
       "  {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0},\n",
       "  {'text': '.afro american you wear a size in a i m at a now still growing Sentence for analyze: This sentence is positive. in some extent we are not exactly overwhelmed against those options choices Sentence for analyze: This sentence is negative.',\n",
       "   'loss_ids': 0,\n",
       "   'shortenable_ids': 0}],\n",
       " {'label': 1}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the first data point\n",
    "first_data_point = train_df.iloc[0]\n",
    "text = first_data_point['Content']\n",
    "label = int(first_data_point['Label'])  # Convert label to int if it's in string format\n",
    "\n",
    "# Create an InputExample object\n",
    "sample_example = InputExample(text_a=text, label=label)\n",
    "\n",
    "# Use wrap_one_example\n",
    "wrapped_example = template.wrap_one_example(sample_example)\n",
    "\n",
    "# Print the wrapped example\n",
    "wrapped_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 3600it [00:02, 1601.91it/s]\n",
      "tokenizing: 3600it [00:01, 1809.51it/s]\n"
     ]
    }
   ],
   "source": [
    "def df_to_inputexamples(df):\n",
    "    input_examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['Content']\n",
    "        label = row['Label']\n",
    "\n",
    "        input_example = InputExample(text_a=text, label=label)\n",
    "        input_examples.append(input_example)\n",
    "    return input_examples\n",
    "\n",
    "\n",
    "# Convert DataFrame to InputExamples\n",
    "train_input_examples = df_to_inputexamples(train_df)\n",
    "valid_input_examples = df_to_inputexamples(valid_df)\n",
    "\n",
    "# Create PromptDataLoaders for training and validation\n",
    "train_dataloader = PromptDataLoader(\n",
    "    dataset=train_input_examples, \n",
    "    template=template, \n",
    "    tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, \n",
    "    decoder_max_length=128, \n",
    "    max_seq_length=128,\n",
    "    batch_size=16, \n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_dataloader = PromptDataLoader(\n",
    "    dataset=valid_input_examples, \n",
    "    template=template, \n",
    "    tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, \n",
    "    decoder_max_length=128, \n",
    "    max_seq_length=128,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sample:  3600\n",
      "Valid Sample:  3600\n"
     ]
    }
   ],
   "source": [
    "print('Train Sample: ', len(train_dataloader) * train_dataloader.batch_size)\n",
    "print('Valid Sample: ', len(valid_dataloader) * valid_dataloader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained and evaluated.\n",
    "        train_dataloader (DataLoader): Dataloader for the training data.\n",
    "        val_dataloader (DataLoader): Dataloader for the validation data.\n",
    "        loss_func (torch.nn.Module): Loss function used for training.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer used for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        float: Best evaluation score achieved during training.\n",
    "    \"\"\"\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # Train the model for one epoch and calculate training loss\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "\n",
    "        # Evaluate the model on the validation set and get the score (accuracy)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "\n",
    "        # Save the model if the current score is better than the best score so far\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(), 'best_model_by_template.pt')\n",
    "\n",
    "        # Print the results for this epoch\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss:.4f}, Eval score={score:.4f}\")\n",
    "\n",
    "    # Return the best score achieved during training\n",
    "    return best_score\n",
    "\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        train_dataloader (DataLoader): Dataloader for the training data.\n",
    "        loss_func (torch.nn.Module): Loss function used for training.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer used for training.\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        # Move batch to the appropriate device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute the logits from the model\n",
    "        outputs = model(batch=batch)\n",
    "\n",
    "        # Extract labels and compute loss\n",
    "        labels = batch['label']\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_all.append(loss.item())\n",
    "\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be evaluated.\n",
    "        val_dataloader (DataLoader): Dataloader for the validation data.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)  # Move inputs to GPU if CUDA is available\n",
    "\n",
    "            # Forward pass: Compute the logits from the model\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # Get the ground truth labels from the inputs\n",
    "            labels = inputs['label']\n",
    "\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "\n",
    "    acc = sum([int(i == j) for i, j in zip(allpreds, alllabels)]) / len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 49/225 [07:32<27:06,  9.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model using the fit function, and store the best score.\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Load the best model state from training for further use or evaluation.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_by_template1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Train the model for one epoch and calculate training loss\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Evaluate the model on the validation set and get the score (accuracy)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     score \u001b[38;5;241m=\u001b[39m evaluate(model, val_dataloader)\n",
      "Cell \u001b[1;32mIn[15], line 58\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_dataloader, loss_func, optimizer)\u001b[0m\n\u001b[0;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Forward pass: Compute the logits from the model\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Extract labels and compute loss\u001b[39;00m\n\u001b[0;32m     61\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openprompt\\pipeline_base.py:300\u001b[0m, in \u001b[0;36mPromptForClassification.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    298\u001b[0m     outputs_at_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_at_mask(output, batch) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     outputs_at_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_at_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m label_words_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbalizer\u001b[38;5;241m.\u001b[39mprocess_outputs(outputs_at_mask, batch\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m label_words_logits\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openprompt\\pipeline_base.py:279\u001b[0m, in \u001b[0;36mPromptForClassification.extract_at_mask\u001b[1;34m(self, outputs, batch)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_at_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m                    outputs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    261\u001b[0m                    batch: Union[Dict, InputFeatures]):\n\u001b[0;32m    262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get outputs at all <mask> token\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    E.g., project the logits of shape\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    (``batch_size``, ``max_seq_length``, ``vocab_size``)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m \n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    280\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an instance of PromptForClassification model.\n",
    "# This model combines the pre-trained language model (PLM) with the defined template and verbalizer.\n",
    "model = PromptForClassification(\n",
    "    copy.deepcopy(plm),  # Deep copy of the pre-trained language model to ensure original is not modified.\n",
    "    template,            # The template that formats the input data for the PLM.\n",
    "    verbalizer           # The verbalizer that maps the PLM's output to specific task labels.\n",
    ")\n",
    "\n",
    "# Define the loss function for the classification task.\n",
    "# CrossEntropyLoss is commonly used for classification problems.\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Parameters that should not undergo weight decay during optimization.\n",
    "# Typically, biases and LayerNorm weights are excluded from weight decay.\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# Grouping model parameters into those that should and shouldn't have weight decay applied.\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Define the optimizer for training, using the AdamW algorithm with grouped parameters.\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "# Determine the device to run the model on (GPU if available, otherwise CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the specified device.\n",
    "model = model.to(device)\n",
    "\n",
    "# Train and evaluate the model using the fit function, and store the best score.\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "# Load the best model state from training for further use or evaluation.\n",
    "model.load_state_dict(torch.load('best_model_by_template1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('tokenizer_hateful_speech')\n",
    "\n",
    "with open('model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(model_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pickle\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('tokenizer_hateful_speech')\n",
    "\n",
    "# Load the saved model_config if needed\n",
    "with open('model_config.pkl', 'rb') as f:\n",
    "    model_config = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PromptForClassification(plm, template, verbalizer)\n",
    "model.load_state_dict(torch.load('best_model_by_template.pt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_tokenizer\\\\tokenizer_config.json',\n",
       " 'my_tokenizer\\\\special_tokens_map.json',\n",
       " 'my_tokenizer\\\\vocab.json',\n",
       " 'my_tokenizer\\\\merges.txt',\n",
       " 'my_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('my_tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
